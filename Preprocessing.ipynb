{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457114f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone = MF, method = TACR-new, dataset = Amazon-Music, val = True\n",
      "lr = 0.0010, lamb = 0.010000, batch_size = 8192, topk = 20\n",
      "tau = 10000000, q = -1.00, b = -2.00\n",
      " lr_q = 0.001000, lr_b = 0.000100\n",
      "high_popularity_quantile = 0.8, smooth_parameter = 256,\n",
      "emb_dim = 64, smooth_parm_alpha = 0.5, dt_u = 10, dt_i = 5, rolling_window = 10\n",
      "model_path: model/Amazon-Music/2024-12-17 21.09.04_MF_TACR-new.pth\n"
     ]
    }
   ],
   "source": [
    "import cppimport\n",
    "import sys\n",
    "sys.path.append('cppcode')\n",
    "\n",
    "from config import opt\n",
    "\n",
    "from models import Models\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from data.dataset import Data\n",
    "import util\n",
    "from datetime import datetime\n",
    "from evaluate import Evaluator\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b30d3bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone = MF, method = TACR-new, dataset = Amazon-Music, val = True\n",
      "lr = 0.0010, lamb = 0.010000, batch_size = 8192, topk = 20\n",
      "tau = 10000000, q = -1.00, b = -2.00\n",
      " lr_q = 0.001000, lr_b = 0.000100\n",
      "high_popularity_quantile = 0.8, smooth_parameter = 256,\n",
      "emb_dim = 64, smooth_parm_alpha = 0.5, dt_u = 10, dt_i = 5, rolling_window = 10\n",
      "model_path: model/Amazon-Music/2024-12-17 21.09.34_MF_TACR-new.pth\n"
     ]
    }
   ],
   "source": [
    "# reload config, so the changed part will be effect\n",
    "import importlib\n",
    "import config\n",
    "import models\n",
    "importlib.reload(config)\n",
    "importlib.reload(models)\n",
    "from config import opt\n",
    "from models import Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce020b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization(data):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    normalized_data = (data - min_val) / (max_val - min_val)\n",
    "    return normalized_data\n",
    "\n",
    "def z_score_normalization(data):\n",
    "    mean_val = np.mean(data)\n",
    "    std_val = np.std(data)\n",
    "    normalized_data = (data - mean_val) / std_val\n",
    "    return normalized_data\n",
    "\n",
    "def max_abs_normalization(data):\n",
    "    max_abs_val = np.max(np.abs(data))\n",
    "    normalized_data = data / max_abs_val\n",
    "    return normalized_data\n",
    "\n",
    "def robust_scaler(data):\n",
    "    median_val = np.median(data)\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    normalized_data = (data - median_val) / iqr\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071566ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon-Music\n"
     ]
    }
   ],
   "source": [
    "dataset_name = opt.dataset # Amazon-Music Ciao Douban-movie\n",
    "\n",
    "print(dataset_name)\n",
    "main_path = 'data/'\n",
    "train_data_path = main_path + '{}/train_data.csv'.format(dataset_name)\n",
    "val_data_path = main_path + '{}/val_data.csv'.format(dataset_name)\n",
    "test_data_path = main_path + '{}/test_data.csv'.format(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "025acd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5541, 3568, 893721600, 1354752000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_data_path, sep='\\t')\n",
    "val_data = pd.read_csv(val_data_path, sep='\\t')\n",
    "test_data = pd.read_csv(test_data_path, sep='\\t')\n",
    "all_data = pd.concat([train_data, val_data, test_data], ignore_index=True)\n",
    "user_num = max(\n",
    "    train_data['user'].max(),\n",
    "    test_data['user'].max(),\n",
    "    val_data['user'].max()) + 1\n",
    "item_num = max(\n",
    "    train_data['item'].max(),\n",
    "    test_data['item'].max(),\n",
    "    val_data['item'].max()) + 1\n",
    "min_time = train_data['timestamp'].min()\n",
    "max_time = train_data['timestamp'].max()\n",
    "\n",
    "user_num, item_num, min_time, max_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e629acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = defaultdict(list)\n",
    "\n",
    "test_user = test_data.user.unique()\n",
    "for i in range(test_data.shape[0]):\n",
    "    uid = test_data.loc[i, 'user']\n",
    "    item_id = test_data.loc[i, 'item']\n",
    "    test_set[uid].append(item_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8c542",
   "metadata": {},
   "source": [
    "### short-term user popularity sensitivity drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06911893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone = MF, method = TACR-new, dataset = Amazon-Music, val = True\n",
      "lr = 0.0010, lamb = 0.010000, batch_size = 8192, topk = 20\n",
      "tau = 10000000, q = -1.00, b = -2.00\n",
      " lr_q = 0.001000, lr_b = 0.000100\n",
      "high_popularity_quantile = 0.8, smooth_parameter = 256,\n",
      "emb_dim = 64, smooth_parm_alpha = 0.5, dt_u = 10, dt_i = 5, rolling_window = 10\n",
      "model_path: model/Amazon-Music/2024-12-17 21.11.06_MF_TACR-new.pth\n"
     ]
    }
   ],
   "source": [
    "# reload config, so the changed part will be effect\n",
    "import importlib\n",
    "import config\n",
    "importlib.reload(config)\n",
    "from config import opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5f1c6a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon-Music\n"
     ]
    }
   ],
   "source": [
    "dataset_name = opt.dataset # Amazon-Music Ciao Douban-movie\n",
    "\n",
    "print(dataset_name)\n",
    "main_path = 'data/'\n",
    "train_data_path = main_path + '{}/train_data.csv'.format(dataset_name)\n",
    "val_data_path = main_path + '{}/val_data.csv'.format(dataset_name)\n",
    "test_data_path = main_path + '{}/test_data.csv'.format(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2b6b7b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5541,\n",
       " 3568,\n",
       " 893721600,\n",
       " 1354752000,\n",
       " datetime.datetime(1998, 4, 28, 9, 0),\n",
       " datetime.datetime(2012, 12, 6, 9, 0),\n",
       " datetime.datetime(1998, 4, 28, 9, 0),\n",
       " datetime.datetime(2014, 7, 23, 9, 0))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_data_path, sep='\\t')\n",
    "val_data = pd.read_csv(val_data_path, sep='\\t')\n",
    "test_data = pd.read_csv(test_data_path, sep='\\t')\n",
    "all_data = pd.concat([train_data, val_data, test_data], ignore_index=True)\n",
    "user_num = max(\n",
    "    train_data['user'].max(),\n",
    "    test_data['user'].max(),\n",
    "    val_data['user'].max()) + 1\n",
    "item_num = max(\n",
    "    train_data['item'].max(),\n",
    "    test_data['item'].max(),\n",
    "    val_data['item'].max()) + 1\n",
    "# print(user_num, item_num, len(train_data) + len(val_data) + len(test_data))\n",
    "min_time = train_data['timestamp'].min()\n",
    "max_time = train_data['timestamp'].max()\n",
    "\n",
    "user_num, item_num, min_time, max_time, \\\n",
    "datetime.fromtimestamp(min_time), datetime.fromtimestamp(max_time), \\\n",
    "datetime.fromtimestamp(all_data['timestamp'].min()), \\\n",
    "datetime.fromtimestamp(all_data['timestamp'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8efeaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data distribution\n",
    "\n",
    "# 统计各个用户的出现频率\n",
    "user_frequency = train_data['user'].value_counts()\n",
    "\n",
    "# 统计各个物品的出现频率\n",
    "item_frequency = train_data['item'].value_counts()\n",
    "\n",
    "items = item_frequency.sort_values().index\n",
    "# item_frequency.sort_values().values\n",
    "all_frequency = item_frequency.unique()\n",
    "\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da38a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "half_year_step_ts = 365//2 * 24 * 60 * 60\n",
    "\n",
    "ts_step = (train_data['timestamp'].max() - train_data['timestamp'].min() - half_year_step_ts)//100\n",
    "\n",
    "all_times = train_data['timestamp'].min() + half_year_step_ts + np.arange(0, 101) * ts_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275d116",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# short-term popularity\n",
    "\n",
    "quantile = 0.8\n",
    "item_frequency_all_times = {}\n",
    "user_frequency_all_times = {}\n",
    "user_frequency_high_popularity_all_times = {}\n",
    "for time_idx in range(len(all_times)):\n",
    "    if time_idx == len(all_times)-1:\n",
    "        cur_ts = all_times[time_idx] - half_year_step_ts\n",
    "        next_ts = train_data['timestamp'].max()\n",
    "    elif time_idx==0:\n",
    "        cur_ts = train_data['timestamp'].min()\n",
    "        next_ts = all_times[time_idx]\n",
    "    else:\n",
    "        cur_ts = all_times[time_idx] - half_year_step_ts\n",
    "        next_ts = all_times[time_idx]\n",
    "    data_between_ts = train_data[np.logical_and(train_data['timestamp'] >= cur_ts, train_data['timestamp'] <= next_ts)]\n",
    "    item_frequency_between_ts = data_between_ts['item'].value_counts()\n",
    "    item_frequency_all_times[time_idx] = item_frequency_between_ts\n",
    "    \n",
    "    user_frequency_between_ts = data_between_ts['user'].value_counts()\n",
    "    user_frequency_all_times[time_idx] = user_frequency_between_ts\n",
    "    \n",
    "    # get the threshold of high_frequency_items\n",
    "    item_high_popularity_threshold = item_frequency_between_ts.quantile(quantile)\n",
    "    high_popularity_items = (item_frequency_between_ts[item_frequency_between_ts>item_high_popularity_threshold]).index\n",
    "    # get the frequency of user's clicks on high_popularity items\n",
    "    data_between_ts_high_popularity = data_between_ts[data_between_ts['item'].isin(high_popularity_items)]\n",
    "    user_frequency_high_popularity_between_ts = data_between_ts_high_popularity['user'].value_counts()\n",
    "    user_frequency_high_popularity_all_times[time_idx] = user_frequency_high_popularity_between_ts\n",
    "    \n",
    "item_frequency_all_times = pd.DataFrame(item_frequency_all_times)\n",
    "user_frequency_all_times = pd.DataFrame(user_frequency_all_times)\n",
    "user_frequency_high_popularity_all_times = pd.DataFrame(user_frequency_high_popularity_all_times)\n",
    "\n",
    "\n",
    "item_frequency_all_times = item_frequency_all_times.reindex(item_frequency.index).sort_index()\n",
    "user_frequency_all_times = user_frequency_all_times.reindex(user_frequency.index).sort_index()\n",
    "user_frequency_high_popularity_all_times = user_frequency_high_popularity_all_times.reindex(user_frequency.index).sort_index()\n",
    "\n",
    "\n",
    "user_frequency_high_popularity_all_times.fillna(0)/(user_frequency_all_times.fillna(0)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5db7948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save short-term item frequency, user frequency, user frequency on high popularity items\n",
    "save_file_name_item_freq = \"data/{}/item_frequency_all_times.csv\".format(dataset_name)\n",
    "save_file_name_user_freq = \"data/{}/user_frequency_all_times.csv\".format(dataset_name)\n",
    "save_file_name_user_freq_high_popu = \"data/{}/user_frequency_high_popularity_all_times.csv\".format(dataset_name)\n",
    "save_file_all_times = \"data/{}/all_times.npy\".format(dataset_name)\n",
    "\n",
    "\n",
    "# print(save_file_name_item_freq)\n",
    "# print(save_file_name_user_freq)\n",
    "# print(save_file_name_user_freq_high_popu)\n",
    "\n",
    "item_frequency_all_times.to_csv(save_file_name_item_freq)\n",
    "user_frequency_all_times.to_csv(save_file_name_user_freq)\n",
    "user_frequency_high_popularity_all_times.to_csv(save_file_name_user_freq_high_popu)\n",
    "np.save(save_file_all_times, all_times)\n",
    "\n",
    "# item_frequency_all_times = pd.read_csv(save_file_name_item_freq, index_col=0)\n",
    "# user_frequency_all_times = pd.read_csv(save_file_name_user_freq, index_col=0)\n",
    "# user_frequency_high_popularity_all_times = pd.read_csv(save_file_name_user_freq_high_popu, index_col=0)\n",
    "# all_times = np.load(save_file_all_times)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tide_env]",
   "language": "python",
   "name": "conda-env-tide_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
